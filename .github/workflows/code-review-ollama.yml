name: AI Code Review

on:
  pull_request:
  push:
    branches: [ main ]
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write

concurrency:
  group: "ai-review"
  cancel-in-progress: true

env:
  MODEL: qwen2.5-coder:7b
  NO_PROXY: "127.0.0.1,localhost"
  # Concurrencia dentro de la acción (2-3 suele ir bien)
  REVIEW_MAX_CONCURRENCY: "2"

jobs:
  review:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    steps:
      - uses: actions/checkout@v4

      # 1) Instala la CLI (daemon) de Ollama sin Docker
      - name: Setup Ollama
        uses: ai-action/setup-ollama@v1
        with:
          version: 0.3.14  # opcional, fija versión

      # 2) Cache de modelos para acelerar pulls
      - name: Cache Ollama models
        uses: actions/cache@v4
        with:
          path: ~/.ollama/models
          key: ollama-${{ runner.os }}-${{ env.MODEL }}
          restore-keys: |
            ollama-${{ runner.os }}-

      # 3) Asegura el modelo y “calienta” la API local
      - name: Pull model
        run: |
          ollama pull "${MODEL}"
          ollama list

      - name: Smoke test API
        run: |
          curl -sS -X POST http://127.0.0.1:11434/api/chat \
            -H "Content-Type: application/json" \
            -d '{"model":"'${MODEL}'","stream":false,"messages":[{"role":"user","content":"ok"}],"keep_alive":"10m"}' \
            | head -c 200

      # (Solo PR) lista de archivos cambiados para reducir el scope
      - name: Collect changed files (PR only)
        id: changed
        if: ${{ github.event_name == 'pull_request' }}
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const pr = context.payload.pull_request;
            const per_page = 100;
            let page = 1, files = [];
            while (true) {
              const { data } = await github.rest.pulls.listFiles({
                ...context.repo, pull_number: pr.number, per_page, page
              });
              files.push(...data);
              if (data.length < per_page) break;
              page++;
            }
            const paths = files
              .filter(f => !['removed'].includes(f.status))
              .map(f => f.filename)
              .filter(p => !p.match(/\.(png|jpe?g|gif|webp|pdf|zip|ico|wasm|exe|dll|so)$/i))
              .filter(p => !p.startsWith('ollama-review-report/'));
            fs.writeFileSync('changed-files.txt', paths.join('\n'));
            core.setOutput('count', String(paths.length));
            core.setOutput('path', 'changed-files.txt');

      - name: Debug count
        if: ${{ github.event_name == 'pull_request' }}
        env:
          CHANGED_COUNT: ${{ steps.changed.outputs.count }}
        run: |
          echo "Changed count: ${CHANGED_COUNT:-0}"

      # 4) Ejecuta tu acción (PR: solo cambiados; Push: glob)
      - name: Run Ollama Code Review (PR changed files)
        if: ${{ github.event_name == 'pull_request' && steps.changed.outputs.count != '0' }}
        uses: bernherre/checkwithai/action-ollama-codereview@v1.3.3
        env:
          REVIEW_MAX_CONCURRENCY: ${{ env.REVIEW_MAX_CONCURRENCY }}
        with:
          model: ${{ env.MODEL }}
          server_url: http://127.0.0.1:11434
          file_list_path: ${{ steps.changed.outputs.path }}
          exclude_glob: |
            **/node_modules/**
            **/dist/**
            **/build/**
            **/.next/**
            **/coverage/**
            **/ollama-review-report/**
            action-ollama-codereview/**
            **/*.lock
            package-lock.json
            yarn.lock
            pnpm-lock.yaml
          fail_on_critica: "true"
          retention_days: "7"
          # performance / resiliencia
          request_timeout_ms: "300000"
          max_bytes_per_file: "200000"
          ollama_num_predict: "256"
          ollama_num_ctx: "1536"
          ollama_temperature: "0.3"

      - name: Run Ollama Code Review (glob fallback)
        if: ${{ github.event_name != 'pull_request' || steps.changed.outputs.count == '0' }}
        uses: bernherre/checkwithai/action-ollama-codereview@v1.3.3
        env:
          REVIEW_MAX_CONCURRENCY: ${{ env.REVIEW_MAX_CONCURRENCY }}
        with:
          model: ${{ env.MODEL }}
          server_url: http://127.0.0.1:11434
          file_glob: "src/**/*.{ts,tsx,js,jsx,py,cs,java,go,rs}"
          exclude_glob: |
            **/node_modules/**
            **/dist/**
            **/build/**
            **/.next/**
            **/coverage/**
            **/ollama-review-report/**
            action-ollama-codereview/**
            **/*.lock
            package-lock.json
            yarn.lock
            pnpm-lock.yaml
          fail_on_critica: "true"
          retention_days: "7"
          request_timeout_ms: "300000"
          max_bytes_per_file: "200000"
          ollama_num_predict: "256"
          ollama_num_ctx: "1536"
          ollama_temperature: "0.3"

      - name: Upload review artifact
        uses: actions/upload-artifact@v4
        with:
          name: ollama-review
          path: ollama-review-report/**
          retention-days: 7
      - name: Post sticky comment (safe)
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          message: |
            ✅ Review listo. Descarga el artifact **ollama-review** desde la pestaña *Summary* del job.
             Si tienes GitHub Pages habilitado para este repo, publica el directorio `ollama-review-report/` y comparte esa URL (https://...github.io/tu-repo/).